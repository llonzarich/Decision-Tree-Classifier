{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA7: Decision Trees\n",
    "- Programmer: Lydia Lonzarich\n",
    "- Class: CPSC 322-01, Fall 2025\n",
    "- Programming Assignment #7\n",
    "- Date of current version: 11/26/2025\n",
    "- Description: this notebook implements a decision tree classifier using hte TDIDT algorithm, selecting attributes to split on using entropy, and examines the effect of using different feature subsets for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "from mysklearn.myutils import cross_val_predict\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "# import mysklearn.myevaluation as myevaluation\n",
    "from mysklearn.myevaluation import confusion_matrix\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the Dataset\n",
    "- Includes 500 instances\n",
    "- Has 9 (categorical) predictive features, each with at most 3 values that it can take on\n",
    "- The target class is 'label' and indicates whether a mushroom is EDIBLE or POISONOUS\n",
    "- dataset features: \n",
    "    - cap-color: gray, brown, other\n",
    "    - odor: foul, none, other\n",
    "    - stalk-surface-above-ring: smooth, silky, other\n",
    "    - stalk-surface-below-ring: smooth, silky, other\n",
    "    - stalk-color-above-ring: pink, white, other\n",
    "    - stalk-color-below-ring: pink, white, other\n",
    "    - ring-type: pendant, evanescent, other\n",
    "    - population: several, other\n",
    "    - habitat: wood, grass, other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"input_data/mushroom_reduced.csv\"\n",
    "pytable = MyPyTable()\n",
    "pytable.load_from_file(filename)\n",
    "\n",
    "# convert all integer values to floats.\n",
    "pytable.convert_to_numeric()\n",
    "\n",
    "# convert the dataset to an array.\n",
    "data = np.array(pytable.data, dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Using only the Odor Feature\n",
    "- In this step, I create a decision tree classifier to predict the label of a mushroom, using a subset of the entire mushroom dataset built from only the odor feature. I tested my classifier using stratified k-fold cross validation, with k=10.\n",
    "- The purpose of this step is to give a baseline set of results. It will demonstrate how accurately the model can predict whether a mushroom is poisonous using only 1 feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Results:\n",
      "accuracy =  0.845278450363196\n",
      "error rate =  0.15472154963680387\n",
      "precision =  0.7891374978068526\n",
      "recall:  0.9433333333333334\n",
      "F1-score:  0.8580008749420515\n"
     ]
    }
   ],
   "source": [
    "# \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of the 'odor' column in the table.\n",
    "odor_indices = pytable.column_names.index(\"odor\")\n",
    "label_indices = pytable.column_names.index(\"label\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = [[x] for x in data[:, odor_indices]]\n",
    "y = list(data[:, label_indices])\n",
    "\n",
    "# compute the avg acc and error rate, avg precision, avg recall, and avg F1 over each train/test split of the data.\n",
    "# nb_acc, nb_err_rate, nb_precision, nb_recall, nb_f1, nb_y_trues, nb_y_preds = cross_val_predict(X, y, 10, MyNaiveBayesClassifier, True)\n",
    "tree_acc, tree_err_rate, tree_precision, tree_recall, tree_f1, tree_y_trues, tree_y_preds = cross_val_predict(X, y, 10, MyDecisionTreeClassifier, True)\n",
    "\n",
    "print(\"Naive Bayes Classifier Results:\")\n",
    "print(\"accuracy = \", tree_acc)\n",
    "print(\"error rate = \", tree_err_rate)\n",
    "print(\"precision = \", tree_precision)\n",
    "print(\"recall: \", tree_recall)\n",
    "print(\"F1-score: \", tree_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "- In this step, I create a decision tree classifier to predict the label of a mushroom using a subset of the entire dataset built from 2-5 features. I repeat this process 3-4 times to compare classification performance across different subsets of data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset 1\n",
    "(cap-color + odor + stalk-color-above-ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "SUBSET 1 RESULTS...\n",
      "(cap-color + odor + stalk-color-above-ring)\n",
      "Decision Tree Classifier Results:\n",
      "accuracy =  0.8357661708751298\n",
      "error rate =  0.1642338291248703\n",
      "precision =  0.8682907772304324\n",
      "recall:  0.8016666666666667\n",
      "F1-score:  0.8226902111066531\n",
      "=====================================================\n",
      "\n",
      "\n",
      "=====================================================\n",
      "SUBSET 1 CONFUSION MATRIX...\n",
      "=====================================================\n",
      "Decision Tree Classifier (Stratified 10-Fold Cross Validation Results):\n",
      "+-----------+----------+-------------+---------+-------------------+\n",
      "| class     |   edible |   poisonous |   Total |   Recognition (%) |\n",
      "+===========+==========+=============+=========+===================+\n",
      "| edible    |      221 |          33 |     254 |              87   |\n",
      "+-----------+----------+-------------+---------+-------------------+\n",
      "| poisonous |       49 |         197 |     246 |              80.1 |\n",
      "+-----------+----------+-------------+---------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of relevant columns in the table.\n",
    "cap_color_indices = pytable.column_names.index(\"cap-color\")\n",
    "odor_indices = pytable.column_names.index(\"odor\")\n",
    "stalk_color_above_ring_indices = pytable.column_names.index(\"stalk-color-above-ring\")\n",
    "label_indices = pytable.column_names.index(\"label\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, cap_color_indices], data[:, odor_indices], data[:, stalk_color_above_ring_indices]))\n",
    "X = X.tolist()\n",
    "y = list(data[:, label_indices])\n",
    "\n",
    "# compute the avg acc and error rate, avg precision, avg recall, and avg F1 over each train/test split of the data.\n",
    "# nb_acc, nb_err_rate, nb_precision, nb_recall, nb_f1, nb_y_trues, nb_y_preds = cross_val_predict(X, y, 10, MyNaiveBayesClassifier, True)\n",
    "tree_acc, tree_err_rate, tree_precision, tree_recall, tree_f1, tree_y_trues, tree_y_preds = cross_val_predict(X, y, 10, MyDecisionTreeClassifier, True)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"SUBSET 1 RESULTS...\")\n",
    "print(\"(cap-color + odor + stalk-color-above-ring)\")\n",
    "print(\"Decision Tree Classifier Results:\")\n",
    "print(\"accuracy = \", tree_acc)\n",
    "print(\"error rate = \", tree_err_rate)\n",
    "print(\"precision = \", tree_precision)\n",
    "print(\"recall: \", tree_recall)\n",
    "print(\"F1-score: \", tree_f1)\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"SUBSET 1 CONFUSION MATRIX...\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "labels = [\"edible\", \"poisonous\"]\n",
    "\n",
    "headers = [\"class\", \"edible\", \"poisonous\", \"Total\", \"Recognition (%)\"]\n",
    "\n",
    "print(\"Decision Tree Classifier (Stratified 10-Fold Cross Validation Results):\")\n",
    "tree_matrix = confusion_matrix(tree_y_trues, tree_y_preds, labels) # ==> a list of lists.\n",
    "tree_matrix = np.array(tree_matrix)\n",
    "totals = tree_matrix.sum(axis=1) # get the totals for each class in the table.\n",
    "\n",
    "# initialize a list to store the recognition % for each row. \n",
    "tree_recognition = [] \n",
    "\n",
    "# iterate through each row in the nb_matrix to calculate its recognition %.\n",
    "# recognition: - another measure of how well the model predicted the true label. \n",
    "#              - we look at how close the diagonal values in the cm (row i, column i) is to the the total for a row i. \n",
    "for row in range(len(tree_matrix)):\n",
    "    # if there are instances the current row, calcuate its recognition %: diagonal / row_total * 100 \n",
    "    if totals[row] > 0:\n",
    "        rec = tree_matrix[row, row] / totals[row] * 100\n",
    "    else:\n",
    "        rec = 0\n",
    "\n",
    "    tree_recognition.append(rec)\n",
    "\n",
    "# add the total counts and recognition (%) of each row to cm matrix. \n",
    "completed_tree_cm = []\n",
    "for i, label in enumerate(labels):\n",
    "    # for each row, append the data as: mpg ranking label | the actual data for each ranking | total count for the row | recognition percept for the row\n",
    "    completed_tree_cm.append([label] + list(tree_matrix[i]) + [totals[i], round(tree_recognition[i], 1)])\n",
    "\n",
    "# create the formatted cm. \n",
    "tree_cm_table = tabulate(completed_tree_cm, headers=headers, tablefmt=\"grid\")\n",
    "print(tree_cm_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset 2\n",
    "(cap-color + habitat + population + stalk-color-below-ring)\n",
    "\n",
    "- note: I expect this subset to give poorer performance metrics than subset 1 because it does not include the odor feature, which was found to be a strong feature for mushroom classification in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "SUBSET 2 RESULTS...\n",
      "(cap-color + habitat + population + stalk-color-below-ring)\n",
      "Decision Tree Classifier Results:\n",
      "accuracy =  0.7551712210307853\n",
      "error rate =  0.24482877896921482\n",
      "precision =  0.7796986711454942\n",
      "recall:  0.7108333333333333\n",
      "F1-score:  0.7400429505451034\n",
      "=====================================================\n",
      "\n",
      "\n",
      "=====================================================\n",
      "SUBSET 2 CONFUSION MATRIX...\n",
      "=====================================================\n",
      "Decision Tree Classifier (Stratified 10-Fold Cross Validation Results):\n",
      "+-----------+----------+-------------+---------+-------------------+\n",
      "| class     |   edible |   poisonous |   Total |   Recognition (%) |\n",
      "+===========+==========+=============+=========+===================+\n",
      "| edible    |      203 |          51 |     254 |              79.9 |\n",
      "+-----------+----------+-------------+---------+-------------------+\n",
      "| poisonous |       71 |         175 |     246 |              71.1 |\n",
      "+-----------+----------+-------------+---------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of relevant columns in the table.\n",
    "cap_color_indices = pytable.column_names.index(\"cap-color\")\n",
    "habitat_indices = pytable.column_names.index(\"habitat\")\n",
    "population_indices = pytable.column_names.index(\"population\")\n",
    "stalk_color_below_ring_indices = pytable.column_names.index(\"stalk-color-below-ring\")\n",
    "label_indices = pytable.column_names.index(\"label\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, cap_color_indices], data[:, habitat_indices], data[:, population_indices], data[:, stalk_color_below_ring_indices]))\n",
    "X = X.tolist()\n",
    "y = list(data[:, label_indices])\n",
    "\n",
    "# compute the avg acc and error rate, avg precision, avg recall, and avg F1 over each train/test split of the data.\n",
    "# nb_acc, nb_err_rate, nb_precision, nb_recall, nb_f1, nb_y_trues, nb_y_preds = cross_val_predict(X, y, 10, MyNaiveBayesClassifier, True)\n",
    "tree_acc, tree_err_rate, tree_precision, tree_recall, tree_f1, tree_y_trues, tree_y_preds = cross_val_predict(X, y, 10, MyDecisionTreeClassifier, True)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"SUBSET 2 RESULTS...\")\n",
    "print(\"(cap-color + habitat + population + stalk-color-below-ring)\")\n",
    "print(\"Decision Tree Classifier Results:\")\n",
    "print(\"accuracy = \", tree_acc)\n",
    "print(\"error rate = \", tree_err_rate)\n",
    "print(\"precision = \", tree_precision)\n",
    "print(\"recall: \", tree_recall)\n",
    "print(\"F1-score: \", tree_f1)\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"SUBSET 2 CONFUSION MATRIX...\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "labels = [\"edible\", \"poisonous\"]\n",
    "\n",
    "headers = [\"class\", \"edible\", \"poisonous\", \"Total\", \"Recognition (%)\"]\n",
    "\n",
    "print(\"Decision Tree Classifier (Stratified 10-Fold Cross Validation Results):\")\n",
    "tree_matrix = confusion_matrix(tree_y_trues, tree_y_preds, labels) # ==> a list of lists.\n",
    "tree_matrix = np.array(tree_matrix)\n",
    "totals = tree_matrix.sum(axis=1) # get the totals for each class in the table.\n",
    "\n",
    "# initialize a list to store the recognition % for each row. \n",
    "tree_recognition = [] \n",
    "\n",
    "# iterate through each row in the nb_matrix to calculate its recognition %.\n",
    "# recognition: - another measure of how well the model predicted the true label. \n",
    "#              - we look at how close the diagonal values in the cm (row i, column i) is to the the total for a row i. \n",
    "for row in range(len(tree_matrix)):\n",
    "    # if there are instances the current row, calcuate its recognition %: diagonal / row_total * 100 \n",
    "    if totals[row] > 0:\n",
    "        rec = tree_matrix[row, row] / totals[row] * 100\n",
    "    else:\n",
    "        rec = 0\n",
    "\n",
    "    tree_recognition.append(rec)\n",
    "\n",
    "# add the total counts and recognition (%) of each row to cm matrix. \n",
    "completed_tree_cm = []\n",
    "for i, label in enumerate(labels):\n",
    "    # for each row, append the data as: mpg ranking label | the actual data for each ranking | total count for the row | recognition percept for the row\n",
    "    completed_tree_cm.append([label] + list(tree_matrix[i]) + [totals[i], round(tree_recognition[i], 1)])\n",
    "\n",
    "# create the formatted cm. \n",
    "tree_cm_table = tabulate(completed_tree_cm, headers=headers, tablefmt=\"grid\")\n",
    "print(tree_cm_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset 3\n",
    "(odor + habitat + population + stalk-color-below-ring)\n",
    "\n",
    "- note: I expect this subset to give better performance metrics than subset 2 because it includes the same features, but I have included the odor feature in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "SUBSET 3 RESULTS...\n",
      "(odor + habitat + population + stalk-color-below-ring)\n",
      "Decision Tree Classifier Results:\n",
      "accuracy =  0.9272224143894846\n",
      "error rate =  0.07277758561051538\n",
      "precision =  0.9394605475040259\n",
      "recall:  0.9133333333333333\n",
      "F1-score:  0.924702923227678\n",
      "=====================================================\n",
      "\n",
      "\n",
      "=====================================================\n",
      "SUBSET 3 CONFUSION MATRIX...\n",
      "=====================================================\n",
      "Decision Tree Classifier (Stratified 10-Fold Cross Validation Results):\n",
      "+-----------+----------+-------------+---------+-------------------+\n",
      "| class     |   edible |   poisonous |   Total |   Recognition (%) |\n",
      "+===========+==========+=============+=========+===================+\n",
      "| edible    |      239 |          15 |     254 |              94.1 |\n",
      "+-----------+----------+-------------+---------+-------------------+\n",
      "| poisonous |       21 |         225 |     246 |              91.5 |\n",
      "+-----------+----------+-------------+---------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of relevant columns in the table.\n",
    "odor_indices = pytable.column_names.index(\"odor\")\n",
    "habitat_indices = pytable.column_names.index(\"habitat\")\n",
    "population_indices = pytable.column_names.index(\"population\")\n",
    "stalk_color_below_ring_indices = pytable.column_names.index(\"stalk-color-below-ring\")\n",
    "label_indices = pytable.column_names.index(\"label\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, odor_indices], data[:, habitat_indices], data[:, population_indices], data[:, stalk_color_below_ring_indices]))\n",
    "X = X.tolist()\n",
    "y = list(data[:, label_indices])\n",
    "\n",
    "# compute the avg acc and error rate, avg precision, avg recall, and avg F1 over each train/test split of the data.\n",
    "# nb_acc, nb_err_rate, nb_precision, nb_recall, nb_f1, nb_y_trues, nb_y_preds = cross_val_predict(X, y, 10, MyNaiveBayesClassifier, True)\n",
    "tree_acc, tree_err_rate, tree_precision, tree_recall, tree_f1, tree_y_trues, tree_y_preds = cross_val_predict(X, y, 10, MyDecisionTreeClassifier, True)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"SUBSET 3 RESULTS...\")\n",
    "print(\"(odor + habitat + population + stalk-color-below-ring)\")\n",
    "print(\"Decision Tree Classifier Results:\")\n",
    "print(\"accuracy = \", tree_acc)\n",
    "print(\"error rate = \", tree_err_rate)\n",
    "print(\"precision = \", tree_precision)\n",
    "print(\"recall: \", tree_recall)\n",
    "print(\"F1-score: \", tree_f1)\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"SUBSET 3 CONFUSION MATRIX...\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "labels = [\"edible\", \"poisonous\"]\n",
    "\n",
    "headers = [\"class\", \"edible\", \"poisonous\", \"Total\", \"Recognition (%)\"]\n",
    "\n",
    "print(\"Decision Tree Classifier (Stratified 10-Fold Cross Validation Results):\")\n",
    "tree_matrix = confusion_matrix(tree_y_trues, tree_y_preds, labels) # ==> a list of lists.\n",
    "tree_matrix = np.array(tree_matrix)\n",
    "totals = tree_matrix.sum(axis=1) # get the totals for each class in the table.\n",
    "\n",
    "# initialize a list to store the recognition % for each row. \n",
    "tree_recognition = [] \n",
    "\n",
    "# iterate through each row in the nb_matrix to calculate its recognition %.\n",
    "# recognition: - another measure of how well the model predicted the true label. \n",
    "#              - we look at how close the diagonal values in the cm (row i, column i) is to the the total for a row i. \n",
    "for row in range(len(tree_matrix)):\n",
    "    # if there are instances the current row, calcuate its recognition %: diagonal / row_total * 100 \n",
    "    if totals[row] > 0:\n",
    "        rec = tree_matrix[row, row] / totals[row] * 100\n",
    "    else:\n",
    "        rec = 0\n",
    "\n",
    "    tree_recognition.append(rec)\n",
    "\n",
    "# add the total counts and recognition (%) of each row to cm matrix. \n",
    "completed_tree_cm = []\n",
    "for i, label in enumerate(labels):\n",
    "    # for each row, append the data as: mpg ranking label | the actual data for each ranking | total count for the row | recognition percept for the row\n",
    "    completed_tree_cm.append([label] + list(tree_matrix[i]) + [totals[i], round(tree_recognition[i], 1)])\n",
    "\n",
    "# create the formatted cm. \n",
    "tree_cm_table = tabulate(completed_tree_cm, headers=headers, tablefmt=\"grid\")\n",
    "print(tree_cm_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Print Decision Rules\n",
    "- In this step, I print the decision rules inferred from my decision tree classifiers when trained over the entire dataset using the four columns (aka, features) chosen for subset 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF att0 == foul AND att3 == other AND att1 == grass THEN label = poisonous\n",
      "IF att0 == foul AND att3 == other AND att1 == other AND att2 == other THEN label = poisonous\n",
      "IF att0 == foul AND att3 == other AND att1 == other AND att2 == several THEN label = poisonous\n",
      "IF att0 == foul AND att3 == other AND att1 == wood THEN label = poisonous\n",
      "IF att0 == foul AND att3 == pink AND att1 == grass THEN label = poisonous\n",
      "IF att0 == foul AND att3 == pink AND att1 == other AND att2 == other THEN label = poisonous\n",
      "IF att0 == foul AND att3 == pink AND att1 == other AND att2 == several THEN label = poisonous\n",
      "IF att0 == foul AND att3 == pink AND att1 == wood AND att2 == other THEN label = poisonous\n",
      "IF att0 == foul AND att3 == pink AND att1 == wood AND att2 == several THEN label = poisonous\n",
      "IF att0 == foul AND att3 == white AND att1 == grass AND att2 == other THEN label = poisonous\n",
      "IF att0 == foul AND att3 == white AND att1 == grass AND att2 == several THEN label = poisonous\n",
      "IF att0 == foul AND att3 == white AND att1 == other AND att2 == other THEN label = poisonous\n",
      "IF att0 == foul AND att3 == white AND att1 == other AND att2 == several THEN label = poisonous\n",
      "IF att0 == foul AND att3 == white AND att1 == wood THEN label = poisonous\n",
      "IF att0 == none AND att3 == other AND att2 == other THEN label = edible\n",
      "IF att0 == none AND att3 == other AND att2 == several THEN label = edible\n",
      "IF att0 == none AND att3 == pink AND att2 == other THEN label = edible\n",
      "IF att0 == none AND att3 == pink AND att2 == several THEN label = edible\n",
      "IF att0 == none AND att3 == white AND att2 == other AND att1 == grass THEN label = edible\n",
      "IF att0 == none AND att3 == white AND att2 == other AND att1 == other THEN label = edible\n",
      "IF att0 == none AND att3 == white AND att2 == other AND att1 == wood THEN label = edible\n",
      "IF att0 == none AND att3 == white AND att2 == several AND att1 == grass THEN label = poisonous\n",
      "IF att0 == none AND att3 == white AND att2 == several AND att1 == other THEN label = edible\n",
      "IF att0 == none AND att3 == white AND att2 == several AND att1 == wood THEN label = edible\n",
      "IF att0 == other AND att2 == other AND att1 == grass THEN label = edible\n",
      "IF att0 == other AND att2 == other AND att1 == other THEN label = edible\n",
      "IF att0 == other AND att2 == other AND att1 == wood THEN label = poisonous\n",
      "IF att0 == other AND att2 == several AND att1 == grass THEN label = poisonous\n",
      "IF att0 == other AND att2 == several AND att1 == other THEN label = poisonous\n",
      "IF att0 == other AND att2 == several AND att1 == wood THEN label = poisonous\n"
     ]
    }
   ],
   "source": [
    "# \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of relevant columns in the table.\n",
    "odor_indices = pytable.column_names.index(\"odor\")\n",
    "habitat_indices = pytable.column_names.index(\"habitat\")\n",
    "population_indices = pytable.column_names.index(\"population\")\n",
    "stalk_color_below_ring_indices = pytable.column_names.index(\"stalk-color-below-ring\")\n",
    "label_indices = pytable.column_names.index(\"label\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, odor_indices], data[:, habitat_indices], data[:, population_indices], data[:, stalk_color_below_ring_indices]))\n",
    "X = X.tolist()\n",
    "y = list(data[:, label_indices])\n",
    "\n",
    "# train tree on full datset\n",
    "tree = MyDecisionTreeClassifier()\n",
    "tree.fit(X, y)\n",
    "\n",
    "# best subset: 3\n",
    "attribute_names = [\"odor\", \"habitat\", \"population\", \"stalk-color-below-ring\"] \n",
    "class_name = \"label\"\n",
    "\n",
    "tree.print_decision_rules(attribute_names, class_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection: based on these decision rules, determine ways my trees can be pruned.\n",
    "- Note: The following rules seem to be all the rules that allow us to reduce the number of decision rules we have in the raw output. \n",
    "- Note: The goal of pruning is to simplify the decision rules, collapsing unecessary or repetive branches (branches that lead to the same classification).\n",
    "\n",
    "- IF att0 == foul THEN label = poisonous\n",
    "    - ==> This collapses rules 1-14 together. In other words, we can replace rules 1-14 with this rule.\n",
    "\n",
    "\n",
    "- IF att0 == none AND att3 == pink THEN label = edible\n",
    "    - This collapses rules 17 and 18 together. In other words, we can replace rules 17 and 18 with this rule.\n",
    "\n",
    "- IF att0 == none AND att3 = white AND att2 == other THEN label = edible\n",
    "    - This collapses rules 19, 20, 21 together. In other words, we can replace rules 19, 20, and 21 with this rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4 (v3.12.4:8e8a4baf65, Jun  6 2024, 17:33:18) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
